{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning using Rectified Linear Units\n",
    "===\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we explore the performance of a neural net with varying activation functions on an image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "__version__ = '1.0.0'\n",
    "__author__ = 'Abien Fred Agarap'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from models.neural_net import NeuralNet\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the GPU memory growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.astype('float32').reshape(-1, 784) / 255.\n",
    "test_features = test_features.astype('float32').reshape(-1, 784) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.one_hot(train_labels, 10)\n",
    "test_labels = tf.one_hot(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `tf.data.Dataset` object for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 23:35:52.758524 140481895573312 deprecation.py:323] From /home/darth/tf2/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "dataset = dataset.prefetch(4096).shuffle(2048).batch(1024, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our helper functions for training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    softmax_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    return tf.reduce_mean(softmax_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for a training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(features)\n",
    "        train_loss = loss(logits=logits, labels=labels)\n",
    "    gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, dataset, epochs):\n",
    "    epoch_accuracy = []\n",
    "    epoch_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        train_accuracy = []\n",
    "        train_loss = 0\n",
    "        for batch_features, batch_labels in dataset:\n",
    "            batch_features += tf.random.normal(stddev=(1. / (1. + epoch)**0.55), shape=batch_features.shape)\n",
    "            loss = train_step(model, loss_fn, batch_features, batch_labels)\n",
    "            \n",
    "            accuracy = tf.metrics.Accuracy()\n",
    "            predictions = tf.nn.softmax(model(batch_features))\n",
    "            accuracy(tf.argmax(predictions, 1), tf.argmax(batch_labels, 1))\n",
    "            \n",
    "            train_loss += loss\n",
    "            train_accuracy.append(accuracy.result())\n",
    "        \n",
    "        epoch_loss.append(tf.reduce_mean(train_loss))\n",
    "        epoch_accuracy.append(tf.reduce_mean(train_accuracy))\n",
    "        \n",
    "        if (epoch != 0) and ((epoch + 1) % 50 == 0):\n",
    "            print('epoch {}/{} : mean loss = {}, mean accuracy = {}'.format(epoch + 1,\n",
    "                                                                            epochs,\n",
    "                                                                            tf.reduce_mean(train_loss),\n",
    "                                                                            tf.reduce_mean(train_accuracy)))\n",
    "    return epoch_accuracy, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic-based Model\n",
    "\n",
    "#### 2-layer Neural Net\n",
    "\n",
    "We define a 2-layer NN with Logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(num_layers=2, neurons=[512, 512], activation=tf.nn.sigmoid, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50/300 : mean loss = 99.00557708740234, mean accuracy = 0.7573410272598267\n",
      "epoch 100/300 : mean loss = 94.01123809814453, mean accuracy = 0.8447097539901733\n",
      "epoch 150/300 : mean loss = 88.87132263183594, mean accuracy = 0.9380893111228943\n",
      "epoch 200/300 : mean loss = 86.87881469726562, mean accuracy = 0.9682280421257019\n",
      "epoch 250/300 : mean loss = 86.22598266601562, mean accuracy = 0.9786166548728943\n",
      "epoch 300/300 : mean loss = 85.86920928955078, mean accuracy = 0.9838699102401733\n"
     ]
    }
   ],
   "source": [
    "logistic_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.970300018787384\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-layer Neural Net\n",
    "\n",
    "We define a 3-layer NN with Logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(num_layers=3, neurons=[512, 256, 128], activation=tf.nn.sigmoid, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50/300 : mean loss = 99.00557708740234, mean accuracy = 0.7573410272598267\n",
      "epoch 100/300 : mean loss = 94.01123809814453, mean accuracy = 0.8447097539901733\n",
      "epoch 150/300 : mean loss = 88.87132263183594, mean accuracy = 0.9380893111228943\n",
      "epoch 200/300 : mean loss = 86.87881469726562, mean accuracy = 0.9682280421257019\n",
      "epoch 250/300 : mean loss = 86.22598266601562, mean accuracy = 0.9786166548728943\n",
      "epoch 300/300 : mean loss = 85.86920928955078, mean accuracy = 0.9838699102401733\n"
     ]
    }
   ],
   "source": [
    "logistic_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.970300018787384\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-layer Neural Net\n",
    "\n",
    "We define a 5-layer NN with Logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(num_layers=5, neurons=[512, 512, 256, 256, 128], activation=tf.nn.sigmoid, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50/300 : mean loss = 99.00557708740234, mean accuracy = 0.7573410272598267\n",
      "epoch 100/300 : mean loss = 94.01123809814453, mean accuracy = 0.8447097539901733\n",
      "epoch 150/300 : mean loss = 88.87132263183594, mean accuracy = 0.9380893111228943\n",
      "epoch 200/300 : mean loss = 86.87881469726562, mean accuracy = 0.9682280421257019\n",
      "epoch 250/300 : mean loss = 86.22598266601562, mean accuracy = 0.9786166548728943\n",
      "epoch 300/300 : mean loss = 85.86920928955078, mean accuracy = 0.9838699102401733\n"
     ]
    }
   ],
   "source": [
    "logistic_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.970300018787384\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TanH-based Model\n",
    "\n",
    "We define a 2-layer NN with Hyperbolic Tangent activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(units=[512, 512], activation=tf.nn.tanh, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU-based Model\n",
    "\n",
    "We define a 2-layer NN with ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(units=[512, 512], activation=tf.nn.relu, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU-based Model\n",
    "\n",
    "We define a 2-layer NN with Leaky ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(units=[512, 512], activation=tf.nn.leaky_relu, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrelu_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softplus-based Model\n",
    "\n",
    "We define a 2-layer NN with Softplus activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(units=[512, 512], activation=tf.nn.softplus, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU-based Model\n",
    "\n",
    "We define a 2-layer NN with ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(units=[512, 512], activation=tf.nn.elu, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu_performance = train(model, loss_fn, dataset, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.Accuracy()\n",
    "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy : {}'.format(test_accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Performance\n",
    "\n",
    "We lay down the training performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(logistic_performance[0])), logistic_performance[0], label='logistic')\n",
    "plt.plot(range(len(tanh_performance[0])), tanh_performance[0], label='tanh')\n",
    "plt.plot(range(len(relu_performance[0])), relu_performance[0], label='relu')\n",
    "plt.plot(range(len(lrelu_performance[0])), lrelu_performance[0], label='leaky_relu')\n",
    "plt.plot(range(len(softplus_performance[0])), softplus_performance[0], label='softplus')\n",
    "plt.plot(range(len(elu_performance[0])), elu_performance[0], label='elu')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(logistic_performance[1])), logistic_performance[1], label='logistic')\n",
    "plt.plot(range(len(tanh_performance[1])), tanh_performance[1], label='tanh')\n",
    "plt.plot(range(len(relu_performance[1])), relu_performance[1], label='relu')\n",
    "plt.plot(range(len(lrelu_performance[1])), lrelu_performance[1], label='leaky_relu')\n",
    "plt.plot(range(len(softplus_performance[1])), softplus_performance[1], label='softplus')\n",
    "plt.plot(range(len(elu_performance[1])), elu_performance[1], label='elu')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig('mnist_performance.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
